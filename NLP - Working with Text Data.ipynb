{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e5f814-2cad-4a0f-9cef-666a543ae30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Regular Expressions: Used to find the patterns available in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407b28c4-6ee4-4594-8126-fba64f1f694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(patterns,text):\n",
    "    if re.search(patterns,text):\n",
    "        return re.search(patterns,text) # first occurence\n",
    "    else:\n",
    "        return 'Not Found!' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767be8da-416c-4cc2-adfe-aa6c90a8e340",
   "metadata": {},
   "source": [
    "## Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "868896ba-6e85-450c-8c14-99361fb712bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='J'>\n",
      "<re.Match object; span=(0, 1), match='T'>\n",
      "Not Found!\n",
      "<re.Match object; span=(0, 1), match='D'>\n",
      "Not Found!\n",
      "<re.Match object; span=(0, 2), match='Ma'>\n"
     ]
    }
   ],
   "source": [
    "# '^': Indicates start of a string (circumflex)\n",
    "# Case Sensitive\n",
    "# order matters\n",
    "print(find_pattern(\"^J\",\"James\"))\n",
    "print(find_pattern(\"^T\",\"Titan\"))\n",
    "print(find_pattern(\"^U\",\"upgrad\"))\n",
    "print(find_pattern(\"^D\",\"DS\"))\n",
    "print(find_pattern(\"^D\",\"Machine DLearning\"))\n",
    "print(find_pattern(\"^Ma\",\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9da7c53-7497-44fb-87f6-4c865474f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"James\"[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb91bb6f-3dc4-43ce-9910-8573f4472a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 5), match='a'>\n",
      "Not Found!\n",
      "<re.Match object; span=(8, 10), match='sh'>\n",
      "<re.Match object; span=(4, 5), match='s'>\n",
      "<re.Match object; span=(13, 16), match='ing'>\n",
      "Not Found!\n"
     ]
    }
   ],
   "source": [
    "# '$': Indicates end of string (dollar)\n",
    "# Case Sensitive\n",
    "# order matters\n",
    "print(find_pattern(\"a$\",\"India\"))\n",
    "print(find_pattern(\"a$\",\"USA\"))\n",
    "print(find_pattern(\"sh$\",\"Bangladesh\"))\n",
    "print(find_pattern(\"s$\",\"James\"))\n",
    "print(find_pattern(\"ing$\",\"Machine Learning\"))\n",
    "print(find_pattern(\"in$\",\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ad91a-4940-4274-b811-7ca0aa85fc7e",
   "metadata": {},
   "source": [
    "## Quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fafd0b7-2728-4730-90bd-577b966a84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 3), match='abb'>\n",
      "<re.Match object; span=(0, 3), match='abb'>\n",
      "<re.Match object; span=(2, 3), match='c'>\n"
     ]
    }
   ],
   "source": [
    "# '*': Zero or more (0,inf)\n",
    "# case sensitive \n",
    "# order matters\n",
    "print(find_pattern(\"ab*\", \"abc\"))#pattern-> a 1 time and b 0 or more times\n",
    "print(find_pattern(\"ab*\", \"ac\"))\n",
    "print(find_pattern(\"ab*\", \"abbc\"))\n",
    "print(find_pattern(\"ab*\", \"abbcab\"))\n",
    "print(find_pattern('cb*','abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8dc8efe-bb97-4d58-b386-c1870e458720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n"
     ]
    }
   ],
   "source": [
    "# '?': Zero or one [0,1]\n",
    "# case sensitive \n",
    "# order matters\n",
    "print(find_pattern(\"ab?\", \"ac\"))#pattern-> a 1 time and b 0 or one time\n",
    "print(find_pattern(\"ab?\", \"abc\"))\n",
    "print(find_pattern(\"ab?\", \"abbc\"))\n",
    "print(find_pattern(\"ab?\", \"abbcab\"))\n",
    "print(find_pattern(\"ab?\", \"acb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4be059e6-8bc9-4796-af5b-326a05f0fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abb'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"ab*\", \"abbc\"))\n",
    "print(find_pattern(\"ab?\", \"abbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e75b1470-aeae-4da4-a36f-21e31ad699c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found!\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 3), match='abb'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 3), match='aab'>\n"
     ]
    }
   ],
   "source": [
    "# '+': One or more [1,inf)\n",
    "# case sensitive \n",
    "# order matters\n",
    "print(find_pattern(\"ab+\", \"ac\"))\n",
    "print(find_pattern(\"ab+\", \"abc\"))\n",
    "print(find_pattern(\"ab+\", \"abbc\"))\n",
    "print(find_pattern(\"a+b\", \"abbc\"))# a --> 1 or more times and b 1 time\n",
    "print(find_pattern(\"a+b\", \"aabbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "039a7593-73f1-472b-8619-6de9ffe3140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "Not Found!\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"ab*\", \"ac\"))\n",
    "print(find_pattern(\"ab?\", \"ac\"))\n",
    "print(find_pattern(\"ab+\", \"ac\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ced27542-a36d-442e-9443-60f1054eac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found!\n",
      "<re.Match object; span=(0, 3), match='abb'>\n",
      "Not Found!\n",
      "<re.Match object; span=(0, 3), match='abb'>\n"
     ]
    }
   ],
   "source": [
    "# {n}: Matches if a character is present exactly n number of times\n",
    "# {n,m}: Matches if a character is present n to m number of times (m and n both inclusive)\n",
    "# case sensitive \n",
    "# order matters\n",
    "print(find_pattern(\"a{1}b{2}\", \"abc\"))\n",
    "print(find_pattern(\"ab{2}\", \"abbc\"))\n",
    "print(find_pattern(\"a{2}b{2}\", \"abbc\"))\n",
    "print(find_pattern(\"a{1,2}b{2}\", \"abbc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16874405-d6ab-4871-b63c-f1b8093760ec",
   "metadata": {},
   "source": [
    "### Character sets\n",
    "| Pattern  | Matches                                                                                    |\n",
    "|----------|--------------------------------------------------------------------------------------------|\n",
    "| [abc]    | Matches either an a, b or c character                                                      |\n",
    "| [abcABC] | Matches either an a, A, b, B, c or C character                                             |\n",
    "| [a-z]    | Matches any characters between a and z, including a and z                                  |\n",
    "| [A-Z]    | Matches any characters between A and Z, including A and Z                                  |\n",
    "| [a-zA-Z] | Matches any characters between a and z, including a and z ignoring cases of the characters |\n",
    "| [0-9]    | Matches any character which is a number between 0 and 9                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32e3ef-983d-43a6-9597-570e88996dd6",
   "metadata": {},
   "source": [
    "# Check if the given mobile number is valid or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "185e9aad-9e8d-4ca2-bbe0-f7d6ba741c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exact 10 digits number\n",
    "#first digit should not be less than 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63a49d65-0690-48a0-815a-8f2770e438de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1234567890 --> not valid\n",
    "#98765432100 --> not valid\n",
    "#987654321--> not valid \n",
    "#05642587547--> not valid number\n",
    "#9876543211--> valid number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26803fcd-4ed5-4119-8faa-ea5c43f79136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found!\n",
      "Not Found!\n",
      "Not Found!\n",
      "Not Found!\n",
      "<re.Match object; span=(0, 10), match='9876543211'>\n"
     ]
    }
   ],
   "source": [
    "pattern='^[6-9][0-9]{9}$'\n",
    "print(find_pattern(pattern,\"1234567890\"))#Not-Valid\n",
    "print(find_pattern(pattern,\"98765432100\"))#Not-Valid\n",
    "print(find_pattern(pattern,\"987654321\"))#Not-Valid\n",
    "print(find_pattern(pattern,\"05642587547\"))#Not-Valid\n",
    "print(find_pattern(pattern,\"9876543211\"))#Valid\n",
    "#print(find_pattern(pattern,\"xxxxxxxxx\"))#Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d7e24-c0b6-4e0a-ac7a-ec7d7afa97f7",
   "metadata": {},
   "source": [
    "## Basic NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b9cb-4fb9-47ce-a89f-e683173e3637",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b97371a2-a7df-4ea8-aaef-66b78dd51f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock, I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock, I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "507bf597-84f7-4b64-8491-d4149f00cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock,\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "print(document.split())\n",
    "# Rule based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79c603d5-f3fd-45f9-aa96-a279b4766e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Natural Language Tool Kit\n",
    "##language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9894b62d-e783-45ec-a8da-39fdb9cedf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8c40d88-2809-4d74-bb04-1af546614ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shivamgarg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')# model\n",
    "# pre-trained model available in nltk for tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4715e1-3328-49a7-94b2-ae8b8e37288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0b1df89-eca7-420a-8049-dd65e521fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", ',', 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "136ae82d-a91a-46aa-8d7d-e8450b36b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock, I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "words = sent_tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8de31-95a4-49b7-bb1d-7b06298a3b2f",
   "metadata": {},
   "source": [
    "## StopWords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e033812c-ea70-4758-bf12-a7f88b3efeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shivamgarg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d84a7e5-fad6-446a-9b17-16b1af96d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dbbb4f97-1fdc-4ebc-bcb5-16038ca7c590",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e66143f-ddb2-4b70-9ad0-87de5a273344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f7a95ca4-02af-4e6d-a103-f1883654f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "26a279f6-5130-4acf-a83e-dc6af4f7e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"Machine learning is a type of artificial intelligence (AI) that allows systems to learn and improve from experience without being explicitly programmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "08198349-b4d5-4a6d-a784-515fe4a29c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a type of artificial intelligence (AI) that allows systems to learn and improve from experience without being explicitly programmed'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cc309bfc-fd3e-44cd-8446-d1df112a4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'is', 'a', 'type', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'allows', 'systems', 'to', 'learn', 'and', 'improve', 'from', 'experience', 'without', 'being', 'explicitly', 'programmed']\n",
      "----------------------------------\n",
      "['Machine', 'learning', 'type', 'artificial', 'intelligence', '(', 'AI', ')', 'allows', 'systems', 'learn', 'improve', 'experience', 'without', 'explicitly', 'programmed']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(document)\n",
    "words_after_stopwords=[word for word in words if word.lower() not in ls]\n",
    "print(words)\n",
    "print(\"----------------------------------\")\n",
    "print(words_after_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9e0c2-13f5-4c3f-8ee2-0baf9836526f",
   "metadata": {},
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5e89b8b7-67ed-4e7d-887c-6eaf1111fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'type', 'artificial', 'intelligence', 'AI', 'allows', 'systems', 'learn', 'improve', 'experience', 'without', 'explicitly', 'programmed']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "final_words=[word for word in words_after_stopwords if word not in string.punctuation]\n",
    "print(final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6bc39-7ebb-48b7-ba15-3438cbe6f8d8",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da309cac-c57a-4bcd-9af4-1af2e4dbe436",
   "metadata": {},
   "source": [
    "Stemming:\n",
    "1. It is a rule base approach hence ended up giving non-english words.\n",
    "2. It is very fast approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8d88656-095b-4e06-a731-90a4f15cc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"task tasked tasks tasking keys mangoes computing looking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e57da454-964e-42b6-a51c-19acc5f21098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task', 'tasked', 'tasks', 'tasking', 'keys', 'mangoes', 'computing', 'looking']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(sent)\n",
    "words_after_stopwords=[word for word in words if word.lower() not in ls]\n",
    "print(words_after_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "55f3d9c5-efec-4ccb-b1b7-f75c71653db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1d6db31d-efdb-419c-9b40-f03fc4438566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task', 'task', 'task', 'task', 'key', 'mango', 'comput', 'look']\n"
     ]
    }
   ],
   "source": [
    "words_after_stemming=[stemmer.stem(word) for word in words_after_stopwords]\n",
    "print(words_after_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca720386-59f0-49f1-8b42-1513d085f6c0",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810853c4-07c7-4b3f-9d5d-5798f1bd7206",
   "metadata": {},
   "source": [
    "Lemmatization:\n",
    "1. It is a corpus/language model search based approach. It is very lineant approach.\n",
    "2. It is a slow approach. It will never give non-english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "efec1195-1dcf-48c4-876d-18be67a2f21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shivamgarg/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0400f533-1a42-43ea-bbf7-94d1a5b23d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f0b6c1bf-3b06-4f8f-8ffc-32852f89fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task', 'tasked', 'tasks', 'tasking', 'keys', 'mangoes', 'computing', 'looking']\n",
      "['task', 'tasked', 'task', 'tasking', 'key', 'mango', 'computing', 'looking']\n"
     ]
    }
   ],
   "source": [
    "words_after_lemmatization=[wordnet_lemmatizer.lemmatize(word) for word in words_after_stopwords]\n",
    "print(words_after_stopwords)\n",
    "print(words_after_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e7f9d-c5f8-469f-be52-b5684545a7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
